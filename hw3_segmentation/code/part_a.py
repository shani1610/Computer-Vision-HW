# -*- coding: utf-8 -*-
"""Part_A.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x9nrSnqxcqye7KZ1vU1Dk6RC2PNH4obi

**Part 1 - Classic Vs. Deep Learning-based Semantic Segmentation**

In this part you are going to compare classic methods for segmentation to deep learning-based methods. You are also going to see the effect of the
background on classification.
"""

#0 imports
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import cv2
from scipy import signal
import time
import os

#pytorch
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
# import datasets in torchvision
import torchvision.datasets as datasets
# import model zoo in torchvision
import torchvision.models as models
from torchvision import utils

"""1. Load the images in the ./data/frogs and ./data/horses folders and display them."""

#1

items_frogs = os.listdir('./data/frogs')

for each_image in items_frogs:
  if each_image.endswith(".jpg"):
    full_path = "./data/frogs/" + each_image
    im = cv2.imread(full_path)
    image = cv2.cvtColor(im,cv2.COLOR_BGR2RGB)  
    plt.figure()
    plt.imshow(image)
    plt.show()
    plt.title(each_image)
    plt.axis(False)
    plt.grid(False)

items_horses = os.listdir('./data/horses')

for each_image in items_horses:
    full_path = "./data/horses/" + each_image
    im = cv2.imread(full_path)
    image = cv2.cvtColor(im,cv2.COLOR_BGR2RGB)  
    plt.figure()
    plt.imshow(image)
    plt.show()
    plt.title(each_image)
    plt.axis(False)
    plt.grid(False)

"""2. Pick 1 classic method for segmentation and 1 deep learning-based method and segment the given images. Display the results.
Briefly summarize each method you picked and discuss the advantages and disadvantages of each method. In your answer, relate to the
results you received in this section.
Note: the classic method must not use any neural network.
"""

#2
# download and load the pre-trained model
model = torch.hub.load('pytorch/vision:v0.5.0', 'deeplabv3_resnet101', pretrained=True)
# put in inference mode
model.eval();
# define device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")



# deep learning-based method:
def deeplabv3_segmentation(filename, label_idx,model):
    # load an image
    input_image = Image.open(filename)
    # define the pre-processing steps
    # image->tensor, normalization
    preprocess = transforms.Compose([
      transforms.ToTensor(),
      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    # perform pre-processing
    input_tensor = preprocess(input_image)
    input_batch = input_tensor.unsqueeze(0) # create a mini-batch of size 1 as expected by the model
    # send to device
    model = model.to(device)
    input_batch = input_batch.to(device)
    # forward pass
    with torch.no_grad():
      output = model(input_batch)['out'][0]
    output_predictions = output.argmax(0)
    print("output shape: ", output.shape)
    print("output_predictions shape: ", output_predictions.shape)
    # create a color pallette, selecting a color for each class
    palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])
    colors = torch.as_tensor([i for i in range(21)])[:, None] * palette
    colors = (colors % 255).numpy().astype("uint8")
    # plot the semantic segmentation predictions of 21 classes in each color
    r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)
    r.putpalette(colors)
    # plot
    fig = plt.figure(figsize=(15,15))
    ax = fig.add_subplot(111)
    #
    ax.imshow(r)
    ax.set_axis_off()
    labels = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle',
    'bus', 'car', 'cat', 'chair', 'cow',
    'diningtable','dog', 'horse', 'motorbike', 'person',
    'pottedplant','sheep', 'sofa', 'train', 'tvmonitor']
    print(["{}: {}".format(i + 1, labels[i]) for i in range(len(labels))])
    # what labels were recognized?
    np.unique(output_predictions.cpu().numpy())
    # create a mask
    mask = torch.zeros_like(output_predictions).float().to(device)
    mask[output_predictions == label_idx] = 1 # 13 is horse
    masked_img = input_image * mask.unsqueeze(2).byte().cpu().numpy()
    fig = plt.figure(figsize=(15,15))
    ax2 = fig.add_subplot(111)
    ax2.imshow(masked_img)
    ax2.set_axis_off()

for each_image in items_horses:
    full_path = "./data/horses/" + each_image
    deeplabv3_segmentation(full_path, 13, model) # 13 is the label_idx for horse

for each_image in items_frogs:
    full_path = "./data/frogs/" + each_image
    deeplabv3_segmentation(full_path, 3, model) # 3 is the label_idx for bird

#2 
# classic method for segmentation
def grabCut_segmentation(img, rect):
  image = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
  mask = np.zeros(image.shape[:2],np.uint8)
  bgdModel = np.zeros((1,65),np.float64)
  fgdModel = np.zeros((1,65),np.float64)
  cv2.grabCut(image,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)
  mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')
  image = image*mask2[:,:,np.newaxis]
  plt.imshow(mask2),plt.show()
  plt.imshow(image),plt.show()
  return image

full_path_horse1 = "./data/horses/horse1.png"
img_horse1 = cv2.imread(full_path_horse1)
rect_horse1 = (50,20,900,1000)
grabCut_segmentation(img_horse1, rect_horse1)

full_path_horse2 = "./data/horses/horse2.jpg"
img_horse2 = cv2.imread(full_path_horse2)
rect_horse2 = (500,200,500,500)
grabCut_segmentation(img_horse2, rect_horse2)

full_path_frog1 = "./data/frogs/frog1.jpg"
img_frog1 = cv2.imread(full_path_frog1)
rect_frog1 = (100,50,300,300)
grabCut_segmentation(img_frog1, rect_frog1)

full_path_frog2 = "./data/frogs/frog2.jpg"
img_frog2 = cv2.imread(full_path_frog2)
rect_frog2 = (150,150,250,150)
grabCut_segmentation(img_frog2, rect_frog2)

"""3. Pick 3 images (download from the internet or take them yourself) that satisfy the following, and display them:
One image of a living being (human, animal,...).
One image of commonly-used object (car, chair, smartphone, glasses,...).
One image of not-so-commonly-used object (fire extinguisher, satellite,... BE CREATIVE).
"""

#3

full_path_tiger = "./my_data/section3/tiger.jpg"
img_tiger = cv2.imread(full_path_tiger)
image_tiger = cv2.cvtColor(img_tiger,cv2.COLOR_BGR2RGB)  
plt.figure()
plt.imshow(image_tiger)
plt.show()
plt.title('tiger')
plt.axis(False)
plt.grid(False)

full_path_chair = "./my_data/section3/chair.jpg"
img_chair = cv2.imread(full_path_chair)
image_chair = cv2.cvtColor(img_chair,cv2.COLOR_BGR2RGB)  
plt.figure()
plt.imshow(image_chair)
plt.show()
plt.title('chair')
plt.axis(False)
plt.grid(False)


full_path_globes = "./my_data/section3/globes.jpg"
img_globes = cv2.imread(full_path_globes)
image_globes = cv2.cvtColor(img_globes,cv2.COLOR_BGR2RGB)  
plt.figure()
plt.imshow(image_globes)
plt.show()
plt.title('globes')
plt.axis(False)
plt.grid(False)

"""4. Apply each method (one classic and one deep learning-based) on the 3 images. Display the results.
Which method performed better on each image? Describe your thoughts on why one method is better than the other.
"""

#4

full_path_tiger = "./my_data/section3/tiger.jpg"
full_path_chair = "./my_data/section3/chair.jpg"
full_path_globes = "./my_data/section3/globes.jpg"


# deep learning based method

deeplabv3_segmentation(full_path_tiger,8, model)

deeplabv3_segmentation(full_path_chair,9 , model)

deeplabv3_segmentation(full_path_globes, 20 , model)

# classic method

full_path_tiger = "./my_data/section3/tiger.jpg"
img_tiger = cv2.imread(full_path_tiger)
rect_tiger = (0,500,1600,2500)
grabCut_segmentation(img_tiger, rect_tiger)

full_path_chair = "./my_data/section3/chair.jpg"
img_chair = cv2.imread(full_path_chair)
rect_chair = (40,25,300,300)
grabCut_segmentation(img_chair, rect_chair)

full_path_globes = "./my_data/section3/globes.jpg"
img_globes = cv2.imread(full_path_globes)
rect_globes = (220,140,150,150)
grabCut_segmentation(img_globes, rect_globes)

"""5. As you probably have noticed, segmentation can be rough around the edges, i.e., the mask is not perfect and may be noisy around the edges.
What can be done to fix or at least alleviate this problem? Your suggestions can be in pre-processing, inside the segmentation algorithm or in
post-processing

6. Load a pre-trained classifier (which was trained on ImageNet) from: https://pytorch.org/docs/stable/torchvision/models.html#classification
(https://pytorch.org/docs/stable/torchvision/models.html#classification). In the previous assignment you used VGG16, but you can choose a
different one for this assignment.
"""

#6
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model = models.vgg16(pretrained=True, progress=True)
model.to(device)
model.eval()

"""7. Pick an image of an animal in its natural habitat (e.g., cow on grass fields, zebra in the safari, ...). Display the image you chose and feedforward
it to the pre-trained network. What is the network's prediction? To convert from class index to label, use the supplied
./data/imagenet1000_clsidx_to_labels.txt file. You can also use the supplied ./data/cow.jpg or ./data/sheep.jpg .
"""

#7

# vgg16
import json
from urllib.request import urlopen
from PIL import Image
target_url = "https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json"

def preprocess(image,device=torch.device("cpu")):
  transfom_normalize = transforms.Compose([
      transforms.Resize((224,224)),
      transforms.CenterCrop(224),
      transforms.ToTensor(),
      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
  ])
  input_tensor = transfom_normalize(image)
  input_batch = input_tensor.unsqueeze(0).to(device) # create a mini-batch as expected by the model
    # move the input and model to GPU for speed if available
  if torch.cuda.is_available():
      input_batch = input_batch.to('cuda')
      model.to('cuda')
  return input_batch


def forward_pass(input_batch):
  #loading the targets of the image - models
  
  json_data = urlopen(target_url) 
  class_idx = json.load(json_data)

  with torch.no_grad():
    # tensor of shape 1000
    output = model(input_batch)

    # we using softmax to normalize the output
    softmax = torch.nn.functional.softmax(output[0], dim=0)
    softmax_val = max(softmax)
    prob = softmax_val*100
    calssifiact_ind =(((softmax_val == softmax).nonzero())[0][0]).item()
    calssifiact = (class_idx.get(str(calssifiact_ind)))[1]
    print("calssification : {}, with probability {:.2f}%".format(calssifiact,prob))

full_path_mice = "./my_data/section7/mice.jpg"
input_image_mice = Image.open(full_path_mice)
in_batch_mice = preprocess(input_image_mice,device)
forward_pass(in_batch_mice)
im_mice = cv2.imread(full_path_mice)
image_mice = cv2.cvtColor(im_mice,cv2.COLOR_BGR2RGB)  
plt.figure()
plt.imshow(image_mice)
plt.show()
plt.title('mice')
plt.axis(False)
plt.grid(False)

"""8. Segement the animal using one of the methods (classic or deep) and display the result."""

#8

#classic method 
rect_mice1 = (0,1100,2000,1400)
cut_mice = grabCut_segmentation(im_mice, rect_mice1)
#cut_mice_again =  grabCut_segmentation(im_mice, rect_mice1)

"""9. Put the the animal in a different habitat, i.e., use the mask to place the animal on a different background. You can choose any background you
want (which is not the animal's natural habitat). For example, put the cow on a beach ( ./data/beach.jpg for example) or the elaphant/sheep
in a room ( ./data/room.jpg for example). Display the result.
You should submit the final image in the output folder, in addition to putting in your report.
"""

# the background
full_path_kitchen = "./my_data/section7/kitchen.jpg"
input_image_kitchen = Image.open(full_path_kitchen)
im_kitchen = cv2.imread(full_path_kitchen)
image_kitchen = cv2.cvtColor(im_kitchen,cv2.COLOR_BGR2RGB)  #4000,6000

# resize
new_width  = im_mice.shape[1] #4000
new_height = im_mice.shape[0] #2669
resized_kitchen = cv2.resize(image_kitchen,(new_width, new_height),interpolation=cv2.INTER_AREA)

# mice mask
mask = cut_mice.copy()
mask[mask>0] = 255

# saving 
resized_kitchen_PIL = Image.fromarray(resized_kitchen)
cut_mice_PIL = Image.fromarray(cut_mice)

resized_kitchen_PIL.save('resized_kitchen.jpg')
cut_mice_PIL.save('cut_mice.jpg')

mask_PIL = Image.fromarray(mask)
mask_PIL.save('mask.jpg', quality=95)



#kitchen_without_mice = resized_kitchen * mask
#plt.imshow(kitchen_without_mice),plt.show()
#rattatui = kitchen_without_mice + cut_mice_again 
#plt.imshow(rattatui),plt.show()

# open Images
im1 = Image.open('cut_mice.jpg')
im2 = Image.open('resized_kitchen.jpg')
mask_im = Image.open('mask.jpg').convert('L')

# pasting the cut on the background
back_im = im2.copy()
back_im.paste(im1, (0, 0), mask_im)
back_im.save('./output/rattatui.jpg', quality=95)
rattatui = Image.open('./output/rattatui.jpg')
plt.imshow(rattatui),plt.show()

"""10. Feed forward the the new image (e.g. the sheep in the room) to the pre-trained network. Was the prediction different than Q7? If so, discuss the
reasons for that to happen.
"""

#10

# vgg16 prediction
#full_path_ratatui = "./output/output_ratatui.jpg"
#input_image_ratatui = Image.open(full_path_ratatui)
in_batch_ratatui = preprocess(ratatui,device)
forward_pass(in_batch_ratatui)

"""end of part 1"""